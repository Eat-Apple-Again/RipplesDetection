{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig, SegformerImageProcessor\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "from transformers import SamModel, SamProcessor\n",
    "from torch import nn\n",
    "from scipy.ndimage import label, find_objects\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b1 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class MySegFormer_0409(nn.Module):\n",
    "    def __init__(self,num_classes,backbone=\"b1\",id2label=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if id2label is not None:\n",
    "            self.id2label = id2label\n",
    "        else:\n",
    "            self.id2label = {i:str(i) for i in range(self.num_classes)}\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(f\"nvidia/mit-{backbone}\",\n",
    "                                                         num_labels=self.num_classes, \n",
    "                                                         id2label=self.id2label, \n",
    "                                                         label2id={v:k for k,v in self.id2label.items()}\n",
    "                                                         , ignore_mismatched_sizes=True)\n",
    "    def forward(self,x):\n",
    "        y = self.segformer(x)\n",
    "        y = nn.functional.interpolate(y.logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False,antialias=True)        \n",
    "        return {'out':y}\n",
    "\n",
    "# Student Model: Segformer 0409\n",
    "model_name = \"nvidia/mit-b0\"\n",
    "num_classes = 2\n",
    "model_segformer = MySegFormer_0409(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_folder():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    parent_folder = filedialog.askdirectory(title=\"選擇資料夾\")\n",
    "    return parent_folder\n",
    "\n",
    "def extract_number_pth(filename):\n",
    "    return int(filename.split('_')[-1].split('.')[0])\n",
    "\n",
    "def extract_number_dir(filename):\n",
    "    return int(filename.split('_')[-1])\n",
    "\n",
    "def criterion(outputs, labels):\n",
    "    return torch.nn.functional.cross_entropy(outputs, labels.squeeze(1).long())\n",
    "\n",
    "def save_metrics(data, filename):\n",
    "    \"\"\" 儲存數據到Pickle檔案 \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def load_metrics(filename):\n",
    "    \"\"\" 從Pickle檔案讀取數據 \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def test_model(model, test_loader, base_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # for 11 種不同比例的 distillation_loss\n",
    "    all_avg_metrics = []\n",
    "    all_std_metrics = []\n",
    "    distillation_loss_dirs = sorted(os.listdir(base_dir), key=extract_number_dir)\n",
    "    print(\"distillation_loss_dir = \", distillation_loss_dirs)\n",
    "    for distillation_loss_dir in distillation_loss_dirs:\n",
    "        print(\"[Evaluate on Test DataSet] Current distillation_loss is \", distillation_loss_dir)\n",
    "        # for 總共18個 DataSet size\n",
    "        avg_metrics = []\n",
    "        std_metrics = []\n",
    "        weight_dir_path = os.path.join(base_dir, distillation_loss_dir)\n",
    "        weight_files = sorted(os.listdir(weight_dir_path), key=extract_number_pth)\n",
    "        for weight_file in weight_files:\n",
    "            print(\"[Evaluate each weight trained on different DataSet size] weight_file = \", weight_file)\n",
    "            model.load_state_dict(torch.load(os.path.join(weight_dir_path, weight_file)))\n",
    "            model.eval()\n",
    "\n",
    "            total_loss = []\n",
    "            iou_scores = []\n",
    "            accuracies = []\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            f1_scores = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for idx, (image, mask) in enumerate(test_loader):\n",
    "                    image, mask = image.to(device), mask.to(device)\n",
    "                    outputs = model(image)\n",
    "                    pred_mask = outputs['out'].argmax(dim=1)\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = criterion(outputs['out'], mask.squeeze(1).long())\n",
    "                    total_loss.append(loss.item())\n",
    "\n",
    "                    # Convert to binary format for evaluation\n",
    "                    mask_np = mask.cpu().numpy().squeeze(1)\n",
    "                    pred_mask_np = pred_mask.cpu().numpy()\n",
    "\n",
    "                    # Flatten the arrays for comparison\n",
    "                    true_flat = mask_np.flatten()\n",
    "                    pred_flat = pred_mask_np.flatten()\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    iou_scores.append(jaccard_score(true_flat, pred_flat, average='binary', zero_division=0))\n",
    "                    accuracies.append(accuracy_score(true_flat, pred_flat))\n",
    "                    precisions.append(precision_score(true_flat, pred_flat, average='binary', zero_division=0))\n",
    "                    recalls.append(recall_score(true_flat, pred_flat, average='binary', zero_division=0))\n",
    "                    f1_scores.append(f1_score(true_flat, pred_flat, average='binary', zero_division=0))\n",
    "\n",
    "            avg_metric = {\n",
    "                'loss': np.mean(total_loss),\n",
    "                'iou': np.mean(iou_scores),\n",
    "                'accuracy': np.mean(accuracies),    \n",
    "                'precision': np.mean(precisions),\n",
    "                'recall': np.mean(recalls),\n",
    "                'f1': np.mean(f1_scores)\n",
    "            }\n",
    "            avg_metrics.append(avg_metric)\n",
    "\n",
    "            std_metric = {\n",
    "                'loss': np.std(total_loss),\n",
    "                'iou': np.std(iou_scores),\n",
    "                'accuracy': np.std(accuracies),    \n",
    "                'precision': np.std(precisions),\n",
    "                'recall': np.std(recalls),\n",
    "                'f1': np.std(f1_scores)\n",
    "            }\n",
    "            std_metrics.append(std_metric)\n",
    "        all_avg_metrics.append(avg_metrics)\n",
    "        all_std_metrics.append(std_metrics)\n",
    "    return all_avg_metrics, all_std_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/user/Desktop/NAS_data/鱸魚/高雄黃明和/train_0501/test\n"
     ]
    }
   ],
   "source": [
    "# DataSet\n",
    "class SplashDataSet_train_val_0501(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.images_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.masks_dir = os.path.join(self.root_dir, \"annotations\")\n",
    "        # get filenames\n",
    "        self.images_list = sorted(os.listdir(self.images_dir))\n",
    "        self.masks_list = sorted(os.listdir(self.masks_dir))\n",
    "        assert len(self.images_list) == len(self.masks_list), \"Number of images and annotations should be the same.\"\n",
    "\n",
    "        # transform image to 1024*1024\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((1024, 1024)),\n",
    "            transforms.ToTensor(),  # This will scale pixel values to [0, 1]\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get image and annotation file\n",
    "        img_path = os.path.join(self.images_dir, self.images_list[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks_list[idx])\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        # Convert mask to binary 0 and 1\n",
    "        mask = (mask > 0).to(torch.int)\n",
    "        mask = mask[0, None, :, :]\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "    def get_time_category(self, filename):\n",
    "        # my filenames' format is 2024-04-09-03-00-11.png\n",
    "        time_str = filename.split('-')[3:5]\n",
    "        time_obj = datetime.strptime('-'.join(time_str), '%H-%M')\n",
    "        hour = time_obj.hour\n",
    "        if hour < 8:\n",
    "            return 'morning'\n",
    "        elif 8 <= hour <= 16:\n",
    "            return 'day'\n",
    "        else:\n",
    "            return 'evening'\n",
    "\n",
    "root_dir = select_folder()\n",
    "print(root_dir)\n",
    "# root_dir = 'C:/Users/user/Desktop/NAS_data/鱸魚/高雄黃明和/train_0501/test'\n",
    "test_dataset = SplashDataSet_train_val_0501(root_dir=root_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dir = select_folder()\n",
    "\n",
    "# 把 all_avg_metrics 和 all_std_metrics 儲存起來\n",
    "saved_metric_dir =select_folder()\n",
    "filename = 'metrics_data.pkl'\n",
    "file_path = os.path.join(saved_metric_dir, filename)\n",
    "\n",
    "all_avg_metrics, all_std_metrics = test_model(model_segformer, test_loader, weight_dir)\n",
    "\n",
    "# 儲存數據\n",
    "metrics_to_save = {'avg_metrics': all_avg_metrics, 'std_metrics': all_std_metrics}\n",
    "save_metrics(metrics_to_save, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_performance(all_avg_metrics, train_sizes, distillation_losses, metric_names):\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(distillation_losses)))\n",
    "\n",
    "    for metric in metric_names:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for i, (metrics, dl) in enumerate(zip(all_avg_metrics, distillation_losses)):\n",
    "            values = [m[metric] for m in metrics]\n",
    "            plt.plot(train_sizes, values, marker='o', color=colors[i], label=f'Distillation loss {dl}%')\n",
    "\n",
    "        plt.title(f'{metric.capitalize()} Across Different Distillation Losses')\n",
    "        plt.xlabel('Training Size')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.legend(title='Distillation Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "metric_names = ['loss', 'iou', 'accuracy', 'precision', 'recall', 'f1']\n",
    "train_sizes = [5, 10, 15, 20, 25, 30, 50, 60, 70, 80, 90, 100, 120, 150, 180, 200, 250, 300]\n",
    "distillation_losses = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "# all_avg_metrics, all_std_metrics = test_model(model_segformer, test_loader, weight_dir)\n",
    "\n",
    "# 讀取之前儲存的 all_avg_metrics 和 all_std_metrics\n",
    "saved_metric_dir =select_folder()\n",
    "filename = 'metrics_data.pkl'\n",
    "file_path = os.path.join(saved_metric_dir, filename)\n",
    "\n",
    "all_avg_metrics = load_metrics(file_path)\n",
    "print(\"Loaded data:\", all_avg_metrics['avg_metrics'])\n",
    "\n",
    "plot_performance(all_avg_metrics['avg_metrics'], train_sizes, distillation_losses, metric_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_keys = metric_test[0].keys()\n",
    "train_sizes = [5, 10, 15, 20, 25, 30, 50, 60, 70, 80, 90, 100, 120, 150, 180, 200, 250, 300]\n",
    "\n",
    "for key in metrics_keys:\n",
    "    plt.figure()\n",
    "    metric_values = [metrics[key] for metrics in metric_test]\n",
    "    plt.plot(train_sizes, metric_values, marker='o')\n",
    "    plt.title(f'{key} vs Dataset Size')\n",
    "    plt.xlabel('Dataset Size')\n",
    "    plt.ylabel(key)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
