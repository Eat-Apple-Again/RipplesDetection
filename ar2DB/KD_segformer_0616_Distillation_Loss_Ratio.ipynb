{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig, SegformerImageProcessor\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "from transformers import SamModel, SamProcessor\n",
    "from torch import nn\n",
    "from scipy.ndimage import label, find_objects\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime\n",
    "\n",
    "# DataSet\n",
    "class SplashDataSet_train_val_0501(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.images_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.masks_dir = os.path.join(self.root_dir, \"annotations\")\n",
    "        # get filenames\n",
    "        self.images_list = sorted(os.listdir(self.images_dir))\n",
    "        self.masks_list = sorted(os.listdir(self.masks_dir))\n",
    "        assert len(self.images_list) == len(self.masks_list), \"Number of images and annotations should be the same.\"\n",
    "\n",
    "        # transform image to 1024*1024\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((1024, 1024)),\n",
    "            transforms.ToTensor(),  # This will scale pixel values to [0, 1]\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get image and annotation file\n",
    "        img_path = os.path.join(self.images_dir, self.images_list[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks_list[idx])\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        # Convert mask to binary 0 and 1\n",
    "        mask = (mask > 0).to(torch.int)\n",
    "        mask = mask[0, None, :, :]\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "    def get_time_category(self, filename):\n",
    "        # my filenames' format is 2024-04-09-03-00-11.png\n",
    "        time_str = filename.split('-')[3:5]\n",
    "        time_obj = datetime.strptime('-'.join(time_str), '%H-%M')\n",
    "        hour = time_obj.hour\n",
    "        if hour < 8:\n",
    "            return 'morning'\n",
    "        elif 8 <= hour <= 16:\n",
    "            return 'day'\n",
    "        else:\n",
    "            return 'evening'\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "    #print(\"inputs size = \", inputs.size())\n",
    "    # inputs size =  torch.Size([1, 2, 1024, 1024])\n",
    "    #print(\"targets size = \", targets.size())\n",
    "    # targets size =  torch.Size([1, 1024, 1024])\n",
    "    BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "    targets = targets.type(torch.float32)\n",
    "    at = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "    pt = torch.exp(-BCE_loss)\n",
    "    F_loss = at * (1 - pt)**gamma * BCE_loss\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        return torch.mean(F_loss), (1 - pt)**gamma\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(F_loss)\n",
    "    else:\n",
    "        return F_loss\n",
    "\n",
    "# criterion\n",
    "def criterion(outputs, labels):\n",
    "    return torch.nn.functional.cross_entropy(outputs, labels.squeeze(1).long())\n",
    "\n",
    "def KD_criterion(student_outputs, teacher_outputs, labels, teacher_ratio, temperature):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    #print(\"student_outputs size = \", student_outputs.size())\n",
    "    # print(\"student_outputs = \", student_outputs[\"out\"])\n",
    "    #print(\"teacher_outputs size = \", teacher_outputs.size())\n",
    "    # print(\"teacher_outputs = \", teacher_outputs)\n",
    "    #print(\"ground truth size = \", labels.size())\n",
    "    \n",
    "    # Calculate Cross Entropy\n",
    "    # original_loss = torch.nn.functional.cross_entropy(student_outputs, labels.squeeze(1).long())\n",
    "\n",
    "    # Calculate Focal Loss , not sure about alpha and gamme\n",
    "    #original_loss = focal_loss(student_outputs[:,1,:,:], labels.squeeze(1).float(), alpha=0.25, gamma=2.0)\n",
    "    alpha=0.25\n",
    "    gamma=2.0\n",
    "    targets = labels.squeeze(1).float()\n",
    "    BCE_loss = F.binary_cross_entropy_with_logits(student_outputs[:,1,:,:], targets, reduction='none')\n",
    "    targets = targets.type(torch.float32)\n",
    "    at = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "    pt = torch.exp(-BCE_loss)\n",
    "    modulating_number = torch.mean((1 - pt)**gamma)\n",
    "    F_loss = at * modulating_number * BCE_loss\n",
    "    #print(\"modulating_number = \", modulating_number)\n",
    "    # mean\n",
    "    original_loss = torch.mean(F_loss)\n",
    "    #print(\"original loss = \", original_loss)\n",
    "\n",
    "    # Calculate Distillation Loss\n",
    "    soft_teacher_outputs = torch.softmax(teacher_outputs[0, 0, :, :] / temperature, dim=1)\n",
    "    soft_student_outputs = torch.log_softmax(student_outputs[0, 0, :, :] / temperature, dim=1)\n",
    "    distillation_loss = nn.KLDivLoss()(soft_student_outputs.to(device), soft_teacher_outputs.to(device))\n",
    "    #print(\"distillation loss = \", distillation_loss)\n",
    "    \n",
    "    # total loss\n",
    "    #total_loss = modulating_number*((1-teacher_ratio)*original_loss + teacher_ratio*distillation_loss)\n",
    "    total_loss = (1-teacher_ratio)*original_loss + teacher_ratio*distillation_loss*modulating_number\n",
    "    return total_loss\n",
    "\n",
    "# evaluate\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for image, mask in val_loader:\n",
    "            image, mask = image.to(device), mask.to(device)\n",
    "\n",
    "            outputs = model(image)\n",
    "\n",
    "            loss = criterion(outputs['out'], mask)\n",
    "\n",
    "            # Calculate Focal Loss , not sure about alpha and gamme\n",
    "            #print(\"outputs size = \", outputs['out'].size())\n",
    "            #print(\"mask size = \", mask.size())\n",
    "            # outputs['out'] size =  torch.Size([1, 2, 1024, 1024])\n",
    "            # mask size =  torch.Size([1, 1, 1024, 1024])\n",
    "            #loss = focal_loss(outputs['out'][:,1:2,:,:], mask.float())\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    print(f\"Average validation loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "# train every epoch\n",
    "def train_one_epoch(student_model, teacher_model, teacher_image_processor, data_loader, teacher_ratio, temperature, optimizer, device, pbar):\n",
    "    student_model.to(device)\n",
    "    student_model.train()\n",
    "    teacher_model.to(device)\n",
    "    teacher_model.eval()\n",
    "    training_loss = []\n",
    "    for idx, (image, mask) in enumerate(data_loader):\n",
    "        #bbox = [[[get_bounding_box(np.array(mask))]]]\n",
    "        bbox, point = get_bounding_box_and_center(np.array(mask))\n",
    "        #print(\"[train_one_epoch] bbox = \", bbox)\n",
    "        #print(\"[train_one_epoch] point = \", point)\n",
    "        image, mask = image.to(device), mask.to(device)\n",
    "        # image size = torch.Size([1, 3, 1024, 1024])start_step\n",
    "        # mask size = torch.Size([1, 1, 1024, 1024])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # output for student model ----------------------------------------------------------\n",
    "        student_outputs = student_model(image)\n",
    "        # outputs size = torch.Size([1, 2, 1024, 1024])\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        # output for teacher model ----------------------------------------------------------\n",
    "        # Retrieve the image embeddings\n",
    "        # processor\n",
    "        teacher_inputs = teacher_image_processor(image, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        teacher_image_embeddings = teacher_model.get_image_embeddings(teacher_inputs[\"pixel_values\"])\n",
    "        \n",
    "        # 送到processor計算遮罩\n",
    "        if bbox is None:\n",
    "            teacher_inputs = teacher_image_processor(image, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        else:\n",
    "            teacher_inputs = teacher_image_processor(image, input_points=[[[point]]], input_boxes=[[[bbox]]], return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "\n",
    "        teacher_inputs.pop(\"pixel_values\", None)\n",
    "        teacher_inputs.update({\"image_embeddings\": teacher_image_embeddings})\n",
    "\n",
    "        teacher_outputs = teacher_model(**teacher_inputs)\n",
    "        teacher_masks, teacher_output = teacher_image_processor.image_processor.post_process_masks(teacher_outputs.pred_masks.cpu(), teacher_inputs[\"original_sizes\"].cpu(), teacher_inputs[\"reshaped_input_sizes\"].cpu())  \n",
    "        # -----------------------------------------------------------------------------------\n",
    "        #loss = criterion(student_outputs, mask)\n",
    "        loss = KD_criterion(student_outputs['out'], teacher_output[0], mask, teacher_ratio, temperature)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \"\"\"\n",
    "        [train_one_epoch] image size =  torch.Size([1, 3, 1024, 1024])\n",
    "        [train_one_epoch]0 mask size =  torch.Size([1, 1, 1024, 1024])\n",
    "        [train_one_epoch]0 outputs size =  torch.Size([1, 2, 128, 128])\n",
    "        [train_one_epoch]1 outputs size =  torch.Size([1, 2, 1024, 1024])\n",
    "        \"\"\"\n",
    "        training_loss.append(loss.item())\n",
    "        pbar.update(idx + 1, values=[(\"loss\", loss.item())])\n",
    "    return np.mean(np.array(training_loss))\n",
    "\n",
    "# train\n",
    "def train(model, teacher_model, teacher_image_processor, train_loader, val_loader, train_size, save_model, teacher_ratio=0.7, temperature=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device, \":\",torch.cuda.get_device_name(0))\n",
    "\n",
    "    train_losses   = []\n",
    "    val_losses     = []\n",
    "    epochs = 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    pbar = tf.keras.utils.Progbar(target=n_batch, stateful_metrics=None)\n",
    "    ######### weight\n",
    "    # 動態生成儲存模型權重的檔名，加入目前使用的資料集大小的數字\n",
    "    weight_filename = f\"segformer_data_size_{train_size}.pth\"\n",
    "    # 確定weights資料夾是否存在，如果不存在則新增它\n",
    "    #weights_dir = os.path.join(os.getcwd(),\"weights\")\n",
    "    weights_dir = os.path.join(os.getcwd(), f\"weights/weights_KD_segformer_0616/weights_KD_segformer_0616_{int(teacher_ratio*100)}\")\n",
    "    if not os.path.exists(weights_dir):\n",
    "        os.makedirs(weights_dir)\n",
    "    model_pathname = os.path.join(weights_dir, weight_filename)\n",
    "    ######### weight end\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        train_loss = train_one_epoch(model, teacher_model, teacher_image_processor, train_loader, teacher_ratio, temperature, optimizer, device, pbar)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if val_loader is not None:\n",
    "            val_loss = evaluate(model, val_loader, device)\n",
    "            val_losses.append(val_loss)\n",
    "            pbar.update(n_batch, values=[('val_loss', val_loss)])\n",
    "\n",
    "            if val_loss < best_val_loss and save_model:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), model_pathname)\n",
    "                print(f\"Saved model weights to '{model_pathname}'.\")\n",
    "    print(f\"Train loss: {np.mean(train_losses)}, Validation loss: {np.mean(val_losses)}\" if val_loader is not None else f\"Train loss: {np.mean(train_losses)}\")\n",
    "    return {'loss':train_losses, 'val_loss':val_losses}\n",
    "\n",
    "def select_folder():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    parent_folder = filedialog.askdirectory(title=\"選擇資料夾\")\n",
    "    return parent_folder\n",
    "\n",
    "def get_bounding_box(ground_truth_map):\n",
    "  ground_truth_map = ground_truth_map[0, 0, :, :]\n",
    "  #print(\"ground_truth_map = \", ground_truth_map.shape)\n",
    "  # get bounding box from mask\n",
    "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  # add perturbation to bounding box coordinates\n",
    "  H, W = ground_truth_map.shape\n",
    "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox\n",
    "\n",
    "def get_bounding_box_and_center(ground_truth_map):\n",
    "    #print(\"[get_bounding_box_and_center]\")\n",
    "    ground_truth_map = ground_truth_map[0, 0, :, :]\n",
    "    if np.any(ground_truth_map > 0):\n",
    "        # get bounding box from mask\n",
    "        y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = ground_truth_map.shape\n",
    "        x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "        x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "        y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "        y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "        bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        # Identify the largest connected component (largest mask area)\n",
    "        labeled_array, num_features = label(ground_truth_map > 0)\n",
    "        if num_features > 0:\n",
    "            # Find the largest component\n",
    "            max_label = 1 + np.argmax([np.sum(labeled_array == i) for i in range(1, num_features+1)])\n",
    "            # Get the slice for the largest component\n",
    "            largest_component_slice = find_objects(labeled_array == max_label)[0]\n",
    "            yc, xc = largest_component_slice\n",
    "            center_x = xc.start + (xc.stop - xc.start) // 2\n",
    "            center_y = yc.start + (yc.stop - yc.start) // 2\n",
    "            center_point = (center_x, center_y)\n",
    "        else:\n",
    "            center_point = ((x_min + x_max) // 2, (y_min + y_max) // 2)\n",
    "    else:\n",
    "        bbox = None\n",
    "        center_point = None\n",
    "    return bbox, center_point\n",
    "\n",
    "class MySegFormer_0604(nn.Module):\n",
    "    def __init__(self,num_classes,backbone=\"b0\",id2label=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if id2label is not None:\n",
    "            self.id2label = id2label\n",
    "        else:\n",
    "            self.id2label = {i:str(i) for i in range(self.num_classes)}\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(f\"nvidia/mit-{backbone}\",\n",
    "                                                         num_labels=self.num_classes, \n",
    "                                                         id2label=self.id2label, \n",
    "                                                         label2id={v:k for k,v in self.id2label.items()}\n",
    "                                                         , ignore_mismatched_sizes=True)\n",
    "    def forward(self,x):\n",
    "        y = self.segformer(x)\n",
    "        y = nn.functional.interpolate(y.logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False,antialias=True)        \n",
    "        return {'out':y}\n",
    "        # 在conda 環境裡huggingface包好的Segformer有改(modeling_segformer.py)\n",
    "\n",
    "# Student Model: Segformer 0601\n",
    "backbone = \"b0\"\n",
    "num_classes = 2\n",
    "model_segformer = MySegFormer_0604(num_classes, backbone)\n",
    "\n",
    "# Teacher Model: Segment Anything Model\n",
    "model_sam = SamModel.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "processor_sam = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "train_sizes = [5, 10, 15, 20, 25, 30, 50, 60, 70, 80, 90, 100, 120, 150, 180, 200, 250, 300]\n",
    "#train_sizes = [300]\n",
    "# 放所有 fold 的 平均、標準差\n",
    "mean_val_losses = []\n",
    "std_val_losses = []\n",
    "root_dir = select_folder()\n",
    "\n",
    "#root_dir = \"C:/Users/user/Desktop/NAS_data/鱸魚/高雄黃明和/train_0418\"\n",
    "#print(root_dir)\n",
    "\n",
    "# My DataSet, return image, mask\n",
    "train_val_dataset = SplashDataSet_train_val_0501(root_dir=root_dir)\n",
    "labels = [train_val_dataset.get_time_category(filename) for filename in train_val_dataset.images_list]\n",
    "#print(\"indices 1= \", indices)\n",
    "#print(\"label size = \", label.size())\n",
    "#print(\"label = \", len(labels))\n",
    "#print(\"len(train_val_dataset = )\", len(train_val_dataset))\n",
    "for train_size in train_sizes:\n",
    "    # 在前一個大小的資料袋中擴增資料(train+validation)\n",
    "    indices = np.arange(len(train_val_dataset))\n",
    "    indices = indices[:train_size]\n",
    "    # print(\"indices = \", indices)\n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    val_losses = []  # 放每個 fold 的 validation loss\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(indices, [labels[i] for i in indices])):\n",
    "        print(f'FOLD {fold+1}')\n",
    "        print('--------------------------------')\n",
    "        train_idx = indices[train_idx]\n",
    "        val_idx = indices[val_idx]\n",
    "        \"\"\"\n",
    "        for i in indices:\n",
    "            print(\"i = \", i)\n",
    "            print(\"labels[i] = \", labels[i])\n",
    "        print(\"----\")\n",
    "        \"\"\"\n",
    "        \n",
    "        train_subset = Subset(train_val_dataset, train_idx)\n",
    "        val_subset = Subset(train_val_dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=1, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)\n",
    "        # train\n",
    "        # Please replace Diatillation_Loss_Ratio to the teacher_ratio from 0 ~ 1 \n",
    "        teacher_ratio = Diatillation_Loss_Ratio\n",
    "        temperature = 5\n",
    "        lc = train(model_segformer, model_sam, processor_sam, train_loader, val_loader, train_size, True, teacher_ratio, temperature)\n",
    "\n",
    "        val_loss = lc['val_loss']\n",
    "        val_losses.append(val_loss)\n",
    "    mean_val_losses.append(np.mean(val_losses))\n",
    "    std_val_losses.append(np.std(val_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "print(\"train_size size = \", np.array(train_sizes).size)\n",
    "print(\"mean_val_losses size = \", np.array(mean_val_losses).size)\n",
    "print(\"std_val_losses size = \", np.array(std_val_losses).size)\n",
    "print(\"train_size = \", train_sizes)\n",
    "print(\"mean_val_losses = \", mean_val_losses)\n",
    "print(\"std_val_losses = \", std_val_losses)\n",
    "\n",
    "results_dir = os.path.join(os.getcwd(), 'TrainingRecords', 'results_KD_segformer_0616')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "results = {\n",
    "    \"train_sizes\": train_sizes,\n",
    "    \"mean_val_losses\": mean_val_losses,\n",
    "    \"std_val_losses\": std_val_losses\n",
    "}\n",
    "json_path = os.path.join(results_dir, f\"results_KD_segformer_0616_{int(teacher_ratio*100)}.json\")\n",
    "with open(json_path, 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=4)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, mean_val_losses, marker='o', color='black', label=f'distillation_loss_ratio={teacher_ratio}, original_loss_ratio={round(1-teacher_ratio, 2)}')\n",
    "plt.fill_between(train_sizes, np.maximum(0, np.array(mean_val_losses) - np.array(std_val_losses)), \n",
    "                 np.array(mean_val_losses) + np.array(std_val_losses), color='black', alpha=0.3)\n",
    "plt.title('Segformer B0')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Cross_Entropy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(results_dir, f\"results_KD_segformer_0616_{int(teacher_ratio*100)}.png\"))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
