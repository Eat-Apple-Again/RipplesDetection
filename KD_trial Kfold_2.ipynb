{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…ˆimportä¸€äº›library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "# import presets\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from tmp_modules import utils,transforms\n",
    "\n",
    "# from coco_utils import get_coco\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "try:\n",
    "    from pytorch_model_summary import summary\n",
    "except:\n",
    "    !pip install pytorch-model-summary\n",
    "    from pytorch_model_summary import summary\n",
    "\n",
    "try:\n",
    "    from torchviz import make_dot\n",
    "except:\n",
    "    !pip install torchviz\n",
    "    from torchviz import make_dot\n",
    "\n",
    "import transformers\n",
    "try:\n",
    "    import datasets\n",
    "except:\n",
    "    !pip install cchardet\n",
    "    !pip install datasets\n",
    "    import datasets\n",
    "    \n",
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from IPython.display import clear_output, display\n",
    "\n",
    "try:\n",
    "        from thop import profile\n",
    "except:\n",
    "        !pip install thop\n",
    "\n",
    "import torch\n",
    "from transformers import SamModel, SamProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…ˆå®šç¾©ä¸€äº›function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(inputs, target):\n",
    "    losses = {}\n",
    "    for name, x in inputs.items():\n",
    "        losses[name] = nn.functional.cross_entropy(x, target, ignore_index=255)\n",
    "\n",
    "    if len(losses) == 1:\n",
    "        return losses[\"out\"]\n",
    "\n",
    "    return losses[\"out\"] + 0.5 * losses[\"aux\"]\n",
    "\n",
    "def KD_criterion(student_outputs, teacher_outputs, labels, temperature=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \"\"\"\n",
    "    print(\"student_outputs size = \", student_outputs[\"out\"].size())\n",
    "    print(\"student_outputs = \", student_outputs[\"out\"])\n",
    "    print(\"teacher_outputs size = \", teacher_outputs[0].size())\n",
    "    print(\"teacher_outputs = \", teacher_outputs[0])\n",
    "    \"\"\"\n",
    "    # Calculate Cross Entropy\n",
    "    original_loss = nn.functional.cross_entropy(student_outputs[\"out\"], labels, ignore_index=255)\n",
    "\n",
    "    # Calculate Distillation Loss\n",
    "    soft_teacher_outputs = torch.softmax(teacher_outputs[0][0, 0, :, :] / temperature, dim=1)\n",
    "    soft_student_outputs = torch.log_softmax(student_outputs[\"out\"][0, 0, :, :] / temperature, dim=1)\n",
    "    distillation_loss = nn.KLDivLoss()(soft_student_outputs.to(device), soft_teacher_outputs.to(device))\n",
    "\n",
    "    # ç¸½æå¤±ç‚ºåŸå§‹æå¤±åŠ ä¸Šè’¸é¤¾æå¤±\n",
    "    total_loss = 0.3*original_loss + 0.7*distillation_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def evaluate(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    header = \"Test:\"\n",
    "    num_processed_samples = 0\n",
    "    total_loss            = 0\n",
    "    with torch.inference_mode():\n",
    "        for image, target in data_loader:\n",
    "            image, target = image.to(device), target.to(device)\n",
    "\n",
    "            ###\n",
    "            target = target.squeeze(1)\n",
    "            # Convert target to Long type\n",
    "            target = target.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            # output = output[\"out\"]\n",
    "            # confmat.update(target.flatten(), output.argmax(1).flatten())\n",
    "            # FIXME need to take into account that the datasets\n",
    "            # could have been padded in distributed setup\n",
    "            num_processed_samples += image.shape[0]\n",
    "            \n",
    "            del image, target, output, loss\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return total_loss/ num_processed_samples\n",
    "\n",
    "# ç”¨enumerateå»iterate through all the data.\n",
    "def train_one_epoch_01(student_model, teacher_model, processor, criterion, optimizer, data_loader, lr_scheduler, device, pbar, scaler=None):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    training_loss = []\n",
    "    for idx, (image, target) in enumerate(data_loader):\n",
    "        target2 = np.array(target)\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        ### ground truth given by annotation\n",
    "        target = target.squeeze(1)\n",
    "        # Convert target to Long type\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "        ############################################################### Teacher Model : SAM\n",
    "        # Retrieve the image embeddings\n",
    "        # processor\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        image_embeddings = teacher_model.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "\n",
    "        # å¾…åŠ åŠŸèƒ½$$$$$\n",
    "        # setup prompts : target points, bounding box\n",
    "        input_boxes = [[[250, 300, 700, 550]]]\n",
    "        input_points = [[[500, 400], [600, 400]]]\n",
    "        \n",
    "        bbox = [[[get_bounding_box(target2)]]]\n",
    "        # é€åˆ°processorè¨ˆç®—é®ç½©\n",
    "        # inputs = processor(image, input_boxes=[input_boxes], input_points=[input_points], return_tensors=\"pt\").to(device)\n",
    "        #inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        inputs = processor(image, input_boxes=[bbox], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        inputs.pop(\"pixel_values\", None)\n",
    "        inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        masks, teacher_output = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "        \"\"\"\n",
    "        ###############################################################\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            student_output = student_model(image)\n",
    "            sam_outputs = teacher_model(**inputs)\n",
    "            # print(\"sam_outputs = \", sam_outputs)\n",
    "            masks, teacher_output = processor.image_processor.post_process_masks(sam_outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "            #print(\"output-->\", output)\n",
    "            loss = criterion(student_output, teacher_output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        pbar.update(idx, values=[(\"loss\",loss.item())])\n",
    "\n",
    "        training_loss.append(loss.item())\n",
    "\n",
    "        del image, target, student_output, loss\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    '''\n",
    "        except:\n",
    "            print('except: *************')\n",
    "            error_count += 1\n",
    "            if error_count < 10:\n",
    "                model.to('cpu')\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                model.to(device)\n",
    "                model.train()\n",
    "            else:\n",
    "                raise RuntimeError('GPU out of memory error')\n",
    "    '''\n",
    "    return np.mean(np.array(training_loss))\n",
    "\n",
    "def train(student_model, teacher_model, processor, epochs, data_loader, data_loader_valid, early_stop=0, model_pathname=True):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device, \":\",torch.cuda.get_device_name(0))\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "    if hasattr(student_model,'backbone'):\n",
    "        params_to_optimize = [\n",
    "            {\"params\": [p for p in student_model.backbone.parameters() if p.requires_grad]},\n",
    "            {\"params\": [p for p in student_model.classifier.parameters() if p.requires_grad]},\n",
    "        ]\n",
    "    else:\n",
    "        params_to_optimize = [{ \"params\": student_model.parameters() }]\n",
    "\n",
    "    optimizer = torch.optim.Adam(params_to_optimize)\n",
    "    scaler = None #torch.cuda.amp.GradScaler()\n",
    "    iters_per_epoch = len(data_loader)\n",
    "    lr_scheduler    = None # PolynomialLR(optimizer, total_iters=iters_per_epoch * epochs, power=0.9)\n",
    "    training_loss   = []\n",
    "    val_loss        = []\n",
    "    min_val_loss    = np.inf\n",
    "    min_val_epoch   = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    n_batch = len(data_loader)\n",
    "    pbar = tf.keras.utils.Progbar(target=n_batch,stateful_metrics=['val_loss'])\n",
    "\n",
    "    ######### weight\n",
    "    weight_filename = \"segformer.pth\"\n",
    "    # ç¢ºå®šweightsè³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºå®ƒ\n",
    "    weights_dir = os.path.join(os.getcwd(), \"weights\")\n",
    "    if not os.path.exists(weights_dir):\n",
    "        os.makedirs(weights_dir)\n",
    "    model_pathname = os.path.join(weights_dir, weight_filename)\n",
    "    ######### weight end    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "        #tr_loss = train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, device, 10, scaler)\n",
    "        tr_loss = train_one_epoch_01(student_model, teacher_model, processor, KD_criterion, optimizer, data_loader, lr_scheduler, device, pbar, scaler)\n",
    "        \n",
    "        #print(tr_loss)\n",
    "        training_loss.append(tr_loss)\n",
    "\n",
    "        if data_loader_valid is not None:\n",
    "            # def evaluate(model, data_loader, device, num_classes):\n",
    "            # def evaluate01(student_model, teacher_model, processor, data_loader, device, criterion):\n",
    "            v_loss = evaluate(student_model, data_loader_valid, device=device, num_classes=2)\n",
    "            #v_loss = evaluate01(student_model, teacher_model, processor, data_loader_valid, KD_criterion, device=device)\n",
    "            val_loss.append(v_loss)\n",
    "            pbar.update(n_batch, values=[('val_loss', v_loss)])\n",
    "\n",
    "            # save the parameters with the least loss\n",
    "            if v_loss < min_val_loss:\n",
    "                min_val_loss = v_loss\n",
    "                min_val_epoch= epoch\n",
    "                if model_pathname is not None:\n",
    "                    torch.save(student_model.state_dict(),model_pathname)\n",
    "                    print(f\"Saved model weights to '{model_pathname}'.\")\n",
    "\n",
    "            # early stop\n",
    "            if early_stop > 0 and epoch - min_val_epoch >= early_stop:\n",
    "                break\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            checkpoint = {\n",
    "                \"model\": student_model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "            } \n",
    "        else:\n",
    "            checkpoint = {\n",
    "                \"model\": student_model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "            } \n",
    "\n",
    "    if data_loader_valid is not None and model_pathname is not None:\n",
    "        student_model.load_state_dict(torch.load(model_pathname))\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print(f\"Training time {total_time_str}\")\n",
    "    return {'loss':training_loss,'val_loss':val_loss}\n",
    "\n",
    "def showSegmentationResult(model, dataset, num_images=10):\n",
    "    plt.figure(figsize=(16, num_images * 5))\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    for i, idx in enumerate(range(min(len(dataset), num_images))):\n",
    "        image, target = dataset[idx]\n",
    "        outputs = model(torch.unsqueeze(image, 0).to(device))   \n",
    "        labels = torch.squeeze(torch.argmax(outputs['out'].cpu(), dim=1)).numpy()\n",
    "\n",
    "        # Original Image\n",
    "        plt.subplot(num_images, 4, i * 4 + 1)\n",
    "        plt.imshow(np.transpose(torch.squeeze(image).cpu().numpy(), (1, 2, 0)))\n",
    "        plt.axis(False)\n",
    "        plt.title('Original Image')\n",
    "\n",
    "        # Ground Truth Label\n",
    "        plt.subplot(num_images, 4, i * 4 + 2)\n",
    "        plt.imshow(target.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis(False)\n",
    "        plt.title('Ground Truth Label')\n",
    "\n",
    "        # Model Prediction\n",
    "        plt.subplot(num_images, 4, i * 4 + 3)\n",
    "        plt.imshow(labels)\n",
    "        plt.axis(False)\n",
    "        plt.title('Model Prediction')\n",
    "\n",
    "        # Overlay Prediction on Original Image\n",
    "        overlay = image.clone()\n",
    "        overlay[0, labels == 1] = 1\n",
    "        plt.subplot(num_images, 4, i * 4 + 4)\n",
    "        plt.imshow(np.transpose(overlay.squeeze().cpu().numpy(), (1, 2, 0)))\n",
    "        plt.axis(False)\n",
    "        plt.title('Overlay Prediction on Original Image')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"outputs['out'] size = \", outputs['out'].size())\n",
    "    print(outputs['out'])\n",
    "    print(\"labels = \", labels)\n",
    "    return\n",
    "\n",
    "def get_bounding_box(ground_truth_map):\n",
    "  ground_truth_map = ground_truth_map[0, 0, :, :]\n",
    "  #print(\"ground_truth_map = \", ground_truth_map.shape)\n",
    "  # get bounding box from mask\n",
    "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  # add perturbation to bounding box coordinates\n",
    "  H, W = ground_truth_map.shape\n",
    "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®šç¾©Segformerï¼Œç›´æ¥ç”¨åŒ…å¥½çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "    def __init__(self,num_classes,backbone=\"b1\",id2label=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if id2label is not None:\n",
    "            self.id2label = id2label\n",
    "        else:\n",
    "            self.id2label = {i:str(i) for i in range(self.num_classes)}\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(f\"nvidia/mit-{backbone}\",\n",
    "                                                         num_labels=self.num_classes, \n",
    "                                                         id2label=self.id2label, \n",
    "                                                         label2id={v:k for k,v in self.id2label.items()})\n",
    "    def forward(self,x):\n",
    "        y = self.segformer(x)\n",
    "        y = nn.functional.interpolate(y.logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False,antialias=True)        \n",
    "        return {'out':y}\n",
    "\n",
    "class SegFormer01(nn.Module):\n",
    "    def __init__(self,num_classes,backbone=\"b1\",id2label=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if id2label is not None:\n",
    "            self.id2label = id2label\n",
    "        else:\n",
    "            self.id2label = {i:str(i) for i in range(self.num_classes)}\n",
    "        self.segformer = SegformerForSemanticSegmentation\n",
    "    def forward(self,x):\n",
    "        y = self.segformer(x)\n",
    "        y = nn.functional.interpolate(y.logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False,antialias=True)        \n",
    "        return {'out':y}\n",
    "    \n",
    "num_classes = 2\n",
    "segformer_transform   = transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Resize((128,128),antialias=True)])\n",
    "segformer_target_transform = transforms.Compose([lambda x:torch.from_numpy(cv2.resize(np.asarray(x),(128,128),cv2.INTER_NEAREST).astype(np.int64)-1)])\n",
    "# segformer_model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b1\")\n",
    "# segformer_model =SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path=\"C:/Users/user/Desktop/Ripple_KD/weights/segformer.pth\")\n",
    "# segformer_model.load_state_dict()\n",
    "\n",
    "segformer_model = SegFormer(num_classes)\n",
    "\n",
    "print(summary(segformer_model, torch.zeros((1, 3, 1024, 1024)), show_input=True, show_parent_layers=True, max_depth=1))\n",
    "\n",
    "# è¨ˆç®—FLOP\n",
    "flops, params = profile(segformer_model, inputs=(torch.zeros((1, 3, 1024, 1024)),), verbose=False)\n",
    "print(\"FLOPs =\", '{:,.0f}'.format(flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®šç¾©SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################### Teacher Model : SAM\n",
    "# define SAM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "print(summary(sam_model, torch.zeros((1, 3, 1024, 1024)).to(device), show_input=True, show_parent_layers=True, max_depth=1))\n",
    "\"\"\"\n",
    "# è¨ˆç®—FLOP\n",
    "flops, params = profile(sam_model, inputs=(torch.zeros((1, 3, 1024, 1024)),), verbose=False)\n",
    "print(\"FLOPs =\", '{:,.0f}'.format(flops))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class SplashDataSet(Dataset):\n",
    "    def __init__(self, data_dir, train_size, val_size, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.images_dir = os.path.join(data_dir, 'images')\n",
    "        self.annotations_dir = os.path.join(data_dir, 'annotations')\n",
    "        self.images_list = os.listdir(self.images_dir)\n",
    "        self.annotations_list = os.listdir(self.annotations_dir)\n",
    "        assert len(self.images_list) == len(self.annotations_list), \"Number of images and annotations should be the same.\"\n",
    "         # éš¨æ©Ÿé¸æ“‡è¨“ç·´é›†å’Œé©—è­‰é›†\n",
    "        if train_size:\n",
    "            train_indices = np.random.choice(len(self.images_list), train_size, replace=False)\n",
    "            self.images_list = [self.images_list[i] for i in train_indices]\n",
    "            self.annotations_list = [self.annotations_list[i] for i in train_indices]\n",
    "        \n",
    "        if val_size:\n",
    "            val_indices = np.random.choice(len(self.images_list), val_size, replace=False)\n",
    "            self.images_list = [self.images_list[i] for i in val_indices]\n",
    "            self.annotations_list = [self.annotations_list[i] for i in val_indices]\n",
    "        \"\"\"\n",
    "        self.train_indices = train_indices\n",
    "        self.val_indices = val_indices\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Read original image\n",
    "        img_name = os.path.join(self.images_dir, self.images_list[idx])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        # Read mask\n",
    "        mask_name = os.path.join(self.annotations_dir, self.annotations_list[idx])\n",
    "        mask = Image.open(mask_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Convert mask to binary 0 and 1\n",
    "        mask = (mask > 0).to(torch.int)\n",
    "        mask = mask[0, None, :, :]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial \n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# transform the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1024, 1024)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def select_folder():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    parent_folder = filedialog.askdirectory(title=\"é¸æ“‡è³‡æ–™å¤¾\")\n",
    "    return parent_folder\n",
    "\n",
    "data_folder = select_folder()\n",
    "\n",
    "# åˆå§‹åŒ– KFold\n",
    "kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# å®šç¾©ä¸åŒçš„è¨“ç·´é›†å¤§å°\n",
    "train_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]  # æ¯å€‹è¨“ç·´é›†çš„å¤§å°\n",
    "val_size = None  # é©—è­‰é›†å¤§å°\n",
    "\n",
    "# åˆå§‹åŒ–ç©ºçš„åˆ—è¡¨ä¾†å­˜å„²ä¸åŒè¨“ç·´é›†å¤§å°ä¸‹çš„å¹³å‡é©—è­‰æå¤±\n",
    "mean_val_losses = []\n",
    "std_val_losses = []\n",
    "mean_val_losses_KD = []\n",
    "std_val_losses_KD = []\n",
    "\n",
    "# è¿´åœˆéæ­·ä¸åŒçš„è¨“ç·´é›†å¤§å°\n",
    "for train_size in train_sizes:\n",
    "    val_losses = []  # å­˜å„²æ¯å€‹ fold çš„é©—è­‰æå¤±\n",
    "    val_losses_KD = []\n",
    "    \n",
    "    # create SplashDataSet\n",
    "    dataset = SplashDataSet(data_dir=data_folder, train_size=train_size, val_size=val_size, transform=transform)\n",
    "    \n",
    "    # è¿´åœˆéæ­·æ¯ä¸€å€‹ fold\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(dataset)):\n",
    "        print(f'Fold {fold+1}')\n",
    "        \n",
    "        # åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†\n",
    "        train_dataset = Subset(dataset, train_index)\n",
    "        val_dataset = Subset(dataset, val_index)\n",
    "        \n",
    "        # åˆå§‹åŒ– DataLoader\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        # åœ¨é€™è£¡é€²è¡Œæ¨¡å‹çš„è¨“ç·´å’Œè©•ä¼°\n",
    "        lc = train(segformer_model, sam_model, processor, 10, train_loader, val_loader, 0, 1)\n",
    "        lc_KD = train(segformer_model, sam_model, processor, 10, train_loader, val_loader, 0.8, 1)\n",
    "\n",
    "        # å–æœ€å¾Œä¸€å€‹ epoch çš„é©—è­‰æå¤±\n",
    "        val_loss = lc['val_loss'][-1]\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        val_loss_KD = lc_KD['val_loss'][-1]\n",
    "        val_losses_KD.append(val_loss_KD)\n",
    "    \n",
    "    # è¨ˆç®—å¹³å‡é©—è­‰æå¤±ä¸¦æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "    mean_val_loss = np.mean(val_losses)\n",
    "    mean_val_losses.append(mean_val_loss)\n",
    "    std_val_loss = np.std(val_losses)\n",
    "    std_val_losses.append(std_val_loss)\n",
    "\n",
    "    mean_val_loss_KD = np.mean(val_losses_KD)\n",
    "    mean_val_losses_KD.append(mean_val_loss_KD)\n",
    "    std_val_loss_KD = np.std(val_losses_KD)\n",
    "    std_val_losses_KD.append(std_val_loss_KD)\n",
    "\n",
    "# åœ¨é€™è£¡ç¹ªè£½åœ–è¡¨\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, mean_val_losses, marker='o', color='black')\n",
    "plt.plot(train_sizes, mean_val_losses_KD, marker='o', color='blue')\n",
    "plt.title('Mean Validation Loss vs. Training Set Size')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# åœ–1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_sizes, mean_val_losses_KD, marker='o', color='b', label='Mean Validation Loss')\n",
    "plt.fill_between(train_sizes, np.maximum(0, np.array(mean_val_losses_KD) - np.array(std_val_losses_KD)), \n",
    "                 np.array(mean_val_losses_KD) + np.array(std_val_losses_KD), color='skyblue', alpha=0.3)\n",
    "plt.title('Mean Validation Loss vs. Training Set Size (teacher_ratio = 0.8, temperature = 1)')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# åœ–2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_sizes, mean_val_losses, marker='o', color='b', label='Mean Validation Loss')\n",
    "plt.fill_between(train_sizes, np.maximum(0, np.array(mean_val_losses) - np.array(std_val_losses)), \n",
    "                 np.array(mean_val_losses) + np.array(std_val_losses), color='black', alpha=0.3)\n",
    "plt.title('Mean Validation Loss vs. Training Set Size (teacher_ratio = 0, temperature = 1)')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # é¿å…é‡ç–Š\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°å‡ºä¾†çœ‹çœ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"å°å‡ºä¾†çœ‹çœ‹ğŸ‘€\")\n",
    "showSegmentationResult(segformer_model, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "weight_path = \"weights/segformer.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(device)\n",
    "num_classes = 2\n",
    "KD_Segformer = SegFormer01(num_classes)\n",
    "\"\"\"\"\"\"\n",
    "# è¼‰å…¥å·²ç¶“è¨“ç·´å¥½çš„æœ€ä½³æ¬Šé‡\n",
    "if weight_path is not None:\n",
    "    print(f\"Loaded model weights from '{weight_path}'.\")\n",
    "    KD_Segformer.load_state_dict(torch.load(weight_path))\n",
    "else:\n",
    "    print(\"Model pathname is not specified. Cannot load model weights.\")\n",
    "\"\"\"\"\"\"\n",
    "KD_Segformer.to(device)\n",
    "KD_Segformer.eval()\n",
    "\n",
    "png_files = select_folder()\n",
    "for file_path in os.listdir(png_files):\n",
    "    if file_path.endswith(\".png\"):\n",
    "        # è®€ä¸€å¼µä¸€å¼µ frame\n",
    "        raw_image = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = KD_Segformer(torch.unsqueeze(raw_image, 0).to(device))\n",
    "            mask = torch.squeeze(torch.argmax(outputs['out'].cpu(), dim=1)).numpy()\n",
    "\n",
    "        plt.imshow(mask)\n",
    "        plt.axis(False)\n",
    "        plt.title('Model Prediction')\n",
    "\n",
    "        # Overlay Prediction on Original Image\n",
    "        overlay = raw_image.clone()\n",
    "        overlay[0, mask == 1] = 1\n",
    "        plt.imshow(np.transpose(overlay.squeeze().cpu().numpy(), (1, 2, 0)))\n",
    "        plt.axis(False)\n",
    "        plt.title('Overlay Prediction on Original Image')\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "############################################################### Teacher Model : SAM\n",
    "# define SAM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "# Retrieve the image embeddings\n",
    "# processor\n",
    "inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "image_embeddings = model.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "\n",
    "# å¾…åŠ åŠŸèƒ½$$$$$\n",
    "# setup prompts : target points, bounding box\n",
    "input_boxes = [[[250, 300, 700, 550]]]\n",
    "input_points = [[[500, 400], [600, 400]]]\n",
    "\n",
    "# é€åˆ°processorè¨ˆç®—é®ç½©\n",
    "inputs = processor(image, input_boxes=[input_boxes], input_points=[input_points], return_tensors=\"pt\").to(device)\n",
    "# inputs = processor(raw_image, input_points=[input_points], return_tensors=\"pt\").to(device)\n",
    "\n",
    "inputs.pop(\"pixel_values\", None)\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks, teacher_output = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "\n",
    "###############################################################\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
